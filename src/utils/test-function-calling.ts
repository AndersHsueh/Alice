/**
 * Function Calling é›†æˆæµ‹è¯•
 * æµ‹è¯• LLM ä¸å·¥å…·ç³»ç»Ÿçš„å®Œæ•´é›†æˆ
 * 
 * æ³¨æ„ï¼šéœ€è¦ LM Studio è¿è¡Œå¹¶æ”¯æŒ Function Calling
 */

import { LLMClient } from '../core/llm.js';
import { configManager } from '../utils/config.js';
import { toolRegistry, builtinTools } from '../tools/index.js';
import type { Message } from '../types/index.js';

async function testFunctionCalling() {
  console.log('ğŸ§ª Function Calling é›†æˆæµ‹è¯•\n');

  // åˆå§‹åŒ–é…ç½®
  await configManager.init();
  const config = configManager.get();
  const modelConfig = configManager.getDefaultModel();

  if (!modelConfig) {
    console.error('âŒ æœªæ‰¾åˆ°é»˜è®¤æ¨¡å‹é…ç½®');
    return;
  }

  // æ³¨å†Œå·¥å…·
  toolRegistry.registerAll(builtinTools);
  console.log(`âœ“ å·²æ³¨å†Œ ${toolRegistry.getAll().length} ä¸ªå·¥å…·\n`);

  // åŠ è½½ç³»ç»Ÿæç¤ºè¯
  const systemPrompt = await configManager.loadSystemPrompt();

  // åˆ›å»º LLM å®¢æˆ·ç«¯
  const llmClient = new LLMClient(modelConfig, systemPrompt);
  llmClient.enableTools(config);

  console.log(`âœ“ ä½¿ç”¨æ¨¡å‹: ${modelConfig.name} (${modelConfig.model})`);
  console.log(`âœ“ å·¥å…·ç³»ç»Ÿå·²å¯ç”¨\n`);

  // æµ‹è¯•åœºæ™¯ 1: è·å–å½“å‰æ—¶é—´
  console.log('--- æµ‹è¯• 1: è·å–å½“å‰æ—¶é—´ ---');
  const test1Messages: Message[] = [
    {
      role: 'user',
      content: 'ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ',
      timestamp: new Date()
    }
  ];

  try {
    console.log('å‘é€è¯·æ±‚...');
    const response1 = await llmClient.chatWithTools(test1Messages, (record) => {
      console.log(`[${record.toolLabel}] ${record.status}`, record.result?.status || '');
    });
    console.log('\nAI å›å¤:', response1.content);
  } catch (error: any) {
    console.error('é”™è¯¯:', error.message);
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯ Function Calling ä¸æ”¯æŒçš„é”™è¯¯
    if (error.message.includes('tools') || error.message.includes('function')) {
      console.log('\nâš ï¸  LM Studio å¯èƒ½ä¸æ”¯æŒ Function Calling');
      console.log('ğŸ’¡ è¯·ç¡®ä¿ï¼š');
      console.log('   1. LM Studio å·²æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬');
      console.log('   2. ä½¿ç”¨çš„æ¨¡å‹æ”¯æŒ Function Calling');
      console.log('   3. æˆ–ä½¿ç”¨ OpenAI API è¿›è¡Œæµ‹è¯•');
    }
  }

  console.log('\n');

  // æµ‹è¯•åœºæ™¯ 2: æœç´¢æ–‡ä»¶ï¼ˆæµå¼ï¼‰
  console.log('--- æµ‹è¯• 2: æœç´¢æ–‡ä»¶ï¼ˆæµå¼ï¼‰ ---');
  const test2Messages: Message[] = [
    {
      role: 'user',
      content: 'è¿™ä¸ªé¡¹ç›®æœ‰å¤šå°‘ä¸ª TypeScript æ–‡ä»¶ï¼Ÿ',
      timestamp: new Date()
    }
  ];

  try {
    console.log('å‘é€è¯·æ±‚ï¼ˆæµå¼ï¼‰...\n');
    let fullResponse = '';
    
    for await (const chunk of llmClient.chatStreamWithTools(test2Messages, (record) => {
      if (record.status === 'running' && record.result?.status) {
        console.log(`[${record.toolLabel}] ${record.result.status}`);
      } else if (record.status === 'success') {
        console.log(`[${record.toolLabel}] âœ“ å®Œæˆ\n`);
      }
    })) {
      process.stdout.write(chunk);
      fullResponse += chunk;
    }
    
    console.log('\n\nâœ… æµå¼å¯¹è¯å®Œæˆ');
  } catch (error: any) {
    console.error('é”™è¯¯:', error.message);
  }

  console.log('\n--- æµ‹è¯•å®Œæˆ ---');
}

// è¿è¡Œæµ‹è¯•
testFunctionCalling().catch(console.error);
